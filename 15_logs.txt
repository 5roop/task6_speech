██████████████████████████████████████████████████████████████████████████████████    89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏   89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎   89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍   90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌   90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊   90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉   90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████   90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████ 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████100%|███████████████████████████████████████{'eval_loss': inf, 'eval_wer': 0.171284114482771, 'eval_runtime': 821.2035, 'eval_samples_per_second': 9.089, 'eval_steps_per_second': 1.136, 'epoch': 5.15}
                                                                                                                   
 17%|███████████▋                                                        | 2400/13980 [5:31:46<20:49:56,  6.48s/it]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                                            Saving model checkpoint to 15_/checkpoint-2400                         Configuration saved in 15_/checkpoint-2400/config.json
Model weights saved in 15_/checkpoint-2400/pytorch_model.bin
Configuration saved in 15_/checkpoint-2400/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-2000] due to args.save_total_limit
 19%|████████████▌                                                       | 2592/13980 [5:51:40<13:49:05,  4.37s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
{'loss': 0.1384, 'learning_rate': 0.00024894658753709195, 'epoch': 6.01}                                           
 20%|█████████████▌                                                      | 2800/13980 [6:13:28<24:13:44,  7.80s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
 20%|█████████████▌                                                      | 2800/13980 [6:27:12<24:13:44,  7.80s/it{'eval_loss': inf, 'eval_wer': 0.17685199321078013, 'eval_runtime': 824.5529, 'eval_samples_per_second': 9.052, 'eval_steps_per_second': 1.132, 'epoch': 6.01}
Saving model checkpoint to 15_/checkpoint-2800
Configuration saved in 15_/checkpoint-2800/config.json
Model weights saved in 15_/checkpoint-2800/pytorch_model.bin
Configuration saved in 15_/checkpoint-2800/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-2400] due to args.save_total_limit
 22%|███████████████▏                                                    | 3133/13980 [7:01:48<13:21:16,  4.43s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
{'loss': 0.1201, 'learning_rate': 0.00024006676557863498, 'epoch': 6.87}                                           
 23%|███████████████▌                                                    | 3200/13980 [7:08:55<22:11:12,  7.41s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.16284249189514427, 'eval_runtime': 821.328, 'eval_samples_per_second': 9.088, 'eval_steps_per_second': 1.136, 'epoch': 6.87}███████████████████████████████████████| 933/933 [13:32<00:00,  1.43it/s]
 23%|███████████████▌                                                    | 3200/13980 [7:22:37<22:11:12,  7.41s/itSaving model checkpoint to 15_/checkpoint-3200                                                                      
Configuration saved in 15_/checkpoint-3200/config.json
Model weights saved in 15_/checkpoint-3200/pytorch_model.bin
Configuration saved in 15_/checkpoint-3200/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-2800] due to args.save_total_limit
 25%|████████████████▉                                                   | 3474/13980 [7:51:09<13:08:52,  4.51s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 26%|█████████████████▌                                                  | 3600/13980 [8:04:18<15:49:07,  5.49s/it]{'loss': 0.1075, 'learning_rate': 0.00023118694362017804, 'epoch': 7.72}
 26%|█████████████████▌                                                  | 3600/13980 [8:04:18<15:49:07,  5.49s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
 26%|█████████████████▌                                                  | 3600/13980 [8:18:02<15:49:07,  5.49s/it{'eval_loss': inf, 'eval_wer': 0.16600361014072365, 'eval_runtime': 824.4319, 'eval_samples_per_second': 9.054, 'eval_steps_per_second': 1.132, 'epoch': 7.72}
Saving model checkpoint to 15_/checkpoint-3600
Configuration saved in 15_/checkpoint-3600/config.json
Model weights saved in 15_/checkpoint-3600/pytorch_model.bin
Configuration saved in 15_/checkpoint-3600/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-3200] due to args.save_total_limit
 28%|███████████████████▎                                                | 3965/13980 [8:56:07<12:27:38,  4.48s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 29%|███████████████████▍                                                | 4000/13980 [8:59:52<15:56:33,  5.75s/it]{'loss': 0.0995, 'learning_rate': 0.00022230712166172105, 'epoch': 8.58}
 29%|███████████████████▍                                                | 4000/13980 [8:59:52<15:56:33,  5.75s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
 29%|███████████████████▍                                                | 4000/13980 [9:13:37<15:56:33,  5.75s/it{'eval_loss': inf, 'eval_wer': 0.16071412534911497, 'eval_runtime': 824.1874, 'eval_samples_per_second': 9.056, 'eval_steps_per_second': 1.132, 'epoch': 8.58}
Saving model checkpoint to 15_/checkpoint-4000
Configuration saved in 15_/checkpoint-4000/config.json
Model weights saved in 15_/checkpoint-4000/pytorch_model.bin
Configuration saved in 15_/checkpoint-4000/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-3600] due to args.save_total_limit
 31%|█████████████████████▍                                              | 4400/13980 [9:55:22<18:33:46,  6.98s/it]{'loss': 0.0892, 'learning_rate': 0.00021340504451038573, 'epoch': 9.44}
 31%|█████████████████████▍                                              | 4400/13980 [9:55:22<18:33:46,  6.98s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                   {'eval_loss': inf, 'eval_wer': 0.16488105394556052, 'eval_runtime': 827.7929, 'eval_samples_per_second': 9.017, 'eval_steps_per_second': 1.127, 'epoch': 9.44}
 31%|█████████████████████                                              | 4400/13980 [10:09:10<18:33:46,  6.98s/itSaving model checkpoint to 15_/checkpoint-4400                                                                      
Configuration saved in 15_/checkpoint-4400/config.json
Model weights saved in 15_/checkpoint-4400/pytorch_model.bin
Configuration saved in 15_/checkpoint-4400/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-4000] due to args.save_total_limit
 33%|██████████████████████▏                                            | 4631/13980 [10:33:17<11:42:38,  4.51s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 34%|███████████████████████                                            | 4800/13980 [10:50:53<17:56:23,  7.04s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
{'loss': 0.0914, 'learning_rate': 0.00020452522255192876, 'epoch': 10.3}
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
 34%|███████████████████████                                            | 4800/13980 [11:04:36<17:56:23,  7.04s/it]
{'eval_loss': inf, 'eval_wer': 0.1760796745485079, 'eval_runtime': 823.3975, 'eval_samples_per_second': 9.065, 'eval_steps_per_second': 1.133, 'epoch': 10.3}                                   Saving model checkpoint to 15_/checkpoint-4800                                                                      
Configuration saved in 15_/checkpoint-4800/config.json
Model weights saved in 15_/checkpoint-4800/pytorch_model.bin
Configuration saved in 15_/checkpoint-4800/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-4400] due to args.save_total_limit
 37%|████████████████████████▌                                          | 5126/13980 [11:38:22<13:40:26,  5.56s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 37%|████████████████████████▉                                          | 5200/13980 [11:46:00<11:40:48,  4.79s/it]{'loss': 0.2105, 'learning_rate': 0.00019564540059347177, 'epoch': 11.16}
 37%|████████████████████████▉                                          | 5200/13980 [11:46:00<11:40:48,  4.79s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                  {'eval_loss': inf, 'eval_wer': 0.35186299426149276, 'eval_runtime': 822.3787, 'eval_samples_per_second': 9.076, 'eval_steps_per_second': 1.135, 'epoch': 11.16}
 37%|████████████████████████▉                                          | 5200/13980 [11:59:42<11:40:48,  4.79s/itSaving model checkpoint to 15_/checkpoint-5200                                                                      
Configuration saved in 15_/checkpoint-5200/config.json
Model weights saved in 15_/checkpoint-5200/pytorch_model.bin
Configuration saved in 15_/checkpoint-5200/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-4800] due to args.save_total_limit
 39%|██████████████████████████▎                                        | 5500/13980 [12:30:56<11:04:00,  4.70s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 40%|██████████████████████████▊                                        | 5600/13980 [12:41:34<15:03:12,  6.47s/it]{'loss': 0.5417, 'learning_rate': 0.00018676557863501483, 'epoch': 12.02}
 40%|██████████████████████████▊                                        | 5600/13980 [12:41:34<15:03:12,  6.47s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                   {'eval_loss': inf, 'eval_wer': 0.49250581484109096, 'eval_runtime': 821.1141, 'eval_samples_per_second': 9.09, 'eval_steps_per_second': 1.136, 'epoch': 12.02}
 40%|██████████████████████████▊                                        | 5600/13980 [12:55:15<15:03:12,  6.47s/itSaving model checkpoint to 15_/checkpoint-5600                                                                      
Configuration saved in 15_/checkpoint-5600/config.json
Model weights saved in 15_/checkpoint-5600/pytorch_model.bin
Configuration saved in 15_/checkpoint-5600/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-5200] due to args.save_total_limit
 42%|████████████████████████████▎                                      | 5904/13980 [13:26:44<10:13:49,  4.56s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 43%|████████████████████████████▊                                      | 6000/13980 [13:36:56<14:02:23,  6.33s/it]{'loss': 0.3617, 'learning_rate': 0.00017788575667655786, 'epoch': 12.87}
 43%|████████████████████████████▊                                      | 6000/13980 [13:36:56<14:02:23,  6.33s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
 43%|████████████████████████████▊                                      | 6000/13980 [13:50:41<14:02:23,  6.33s/it{'eval_loss': inf, 'eval_wer': 0.2021050173771699, 'eval_runtime': 824.9696, 'eval_samples_per_second': 9.048, 'eval_steps_per_second': 1.131, 'epoch': 12.87}
Saving model checkpoint to 15_/checkpoint-6000
Configuration saved in 15_/checkpoint-6000/config.json
Model weights saved in 15_/checkpoint-6000/pytorch_model.bin
Configuration saved in 15_/checkpoint-6000/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-5600] due to args.save_total_limit
 44%|█████████████████████████████▍                                     | 6132/13980 [14:04:29<10:36:41,  4.87s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
                                                                                                                   {'loss': 0.2969, 'learning_rate': 0.00016900593471810087, 'epoch': 13.73}
 46%|██████████████████████████████▋                                    | 6400/13980 [14:32:37<15:04:16,  7.16s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                  {'eval_loss': inf, 'eval_wer': 0.19662694314477383, 'eval_runtime': 825.6692, 'eval_samples_per_second': 9.04, 'eval_steps_per_second': 1.13, 'epoch': 13.73}
 46%|██████████████████████████████▋                                    | 6400/13980 [14:46:22<15:04:16,  7.16s/itSaving model checkpoint to 15_/checkpoint-6400                                                                      
Configuration saved in 15_/checkpoint-6400/config.json
Model weights saved in 15_/checkpoint-6400/pytorch_model.bin
Configuration saved in 15_/checkpoint-6400/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-6000] due to args.save_total_limit
 48%|████████████████████████████████▌                                   | 6686/13980 [15:15:55<8:55:43,  4.41s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
{'loss': 0.6547, 'learning_rate': 0.0001601261127596439, 'epoch': 14.59}                                           
 49%|████████████████████████████████▌                                  | 6800/13980 [15:27:42<12:09:04,  6.09s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.24660314495343638, 'eval_runtime': 822.4787, 'eval_samples_per_second': 9.075, 'eval_steps_per_second': 1.134, 'epoch': 14.59}█████████████████████████████████████| 933/933 [13:33<00:00,  1.41it/s]
 49%|████████████████████████████████▌                                  | 6800/13980 [15:41:24<12:09:04,  6.09s/itSaving model checkpoint to 15_/checkpoint-6800                                                                      
Configuration saved in 15_/checkpoint-6800/config.json
Model weights saved in 15_/checkpoint-6800/pytorch_model.bin
Configuration saved in 15_/checkpoint-6800/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-6400] due to args.save_total_limit
 52%|██████████████████████████████████▌                                | 7200/13980 [16:22:55<10:19:16,  5.48s/it]{'loss': 0.4897, 'learning_rate': 0.00015122403560830858, 'epoch': 15.45}
The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                  {'eval_loss': inf, 'eval_wer': 0.2110136233419845, 'eval_runtime': 822.0326, 'eval_samples_per_second': 9.08, 'eval_steps_per_second': 1.135, 'epoch': 15.45}
 52%|██████████████████████████████████▌                                | 7200/13980 [16:36:37<10:19:16,  5.48s/itSaving model checkpoint to 15_/checkpoint-7200                                                                      
Configuration saved in 15_/checkpoint-7200/config.json
Model weights saved in 15_/checkpoint-7200/pytorch_model.bin
Configuration saved in 15_/checkpoint-7200/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-6800] due to args.save_total_limit
 52%|███████████████████████████████████▌                                | 7314/13980 [16:48:21<8:49:06,  4.76s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 54%|████████████████████████████████████▍                              | 7600/13980 [17:18:08<11:51:16,  6.69s/it]{'loss': 0.5019, 'learning_rate': 0.00014236646884272996, 'epoch': 16.31}
 54%|████████████████████████████████████▍                              | 7600/13980 [17:18:08<11:51:16,  6.69s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.28777850619201994, 'eval_runtime': 828.6144, 'eval_samples_per_second': 9.008, 'eval_steps_per_second': 1.126, 'epoch': 16.31}                                                                       
 54%|████████████████████████████████████▍                              | 7600/13980 [17:31:57<11:51:16,  6.69s/itSaving model checkpoint to 15_/checkpoint-7600                                                                      
Configuration saved in 15_/checkpoint-7600/config.json
Model weights saved in 15_/checkpoint-7600/pytorch_model.bin
Configuration saved in 15_/checkpoint-7600/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-7200] due to args.save_total_limit
{'loss': 0.4681, 'learning_rate': 0.00013346439169139465, 'epoch': 17.17}                                          
 57%|██████████████████████████████████████▎                            | 8000/13980 [18:13:24<12:13:09,  7.36s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                  {'eval_loss': inf, 'eval_wer': 0.2513538027713667, 'eval_runtime': 822.2589, 'eval_samples_per_second': 9.077, 'eval_steps_per_second': 1.135, 'epoch': 17.17}
 57%|██████████████████████████████████████▎                            | 8000/13980 [18:27:07<12:13:09,  7.36s/itSaving model checkpoint to 15_/checkpoint-8000                                                                      
Configuration saved in 15_/checkpoint-8000/config.json
Model weights saved in 15_/checkpoint-8000/pytorch_model.bin
Configuration saved in 15_/checkpoint-8000/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-7600] due to args.save_total_limit
 58%|███████████████████████████████████████▎                            | 8084/13980 [18:35:44<6:52:30,  4.20s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 60%|████████████████████████████████████████▊                           | 8400/13980 [19:08:26<6:42:06,  4.32s/it]{'loss': 0.3683, 'learning_rate': 0.00012458456973293768, 'epoch': 18.03}
The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                  {'eval_loss': inf, 'eval_wer': 0.23253078049087136, 'eval_runtime': 821.7773, 'eval_samples_per_second': 9.083, 'eval_steps_per_second': 1.135, 'epoch': 18.03}
 60%|████████████████████████████████████████▊                           | 8400/13980 [19:22:08<6:42:06,  4.32s/itSaving model checkpoint to 15_/checkpoint-8400                                                                      
Configuration saved in 15_/checkpoint-8400/config.json
Model weights saved in 15_/checkpoint-8400/pytorch_model.bin
Configuration saved in 15_/checkpoint-8400/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-8000] due to args.save_total_limit
 62%|██████████████████████████████████████████▎                         | 8700/13980 [19:53:06<6:04:58,  4.15s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 63%|██████████████████████████████████████████▊                         | 8800/13980 [20:03:26<6:27:09,  4.48s/it]{'loss': 0.3134, 'learning_rate': 0.0001157047477744807, 'epoch': 18.88}
 63%|██████████████████████████████████████████▊                         | 8800/13980 [20:03:26<6:27:09,  4.48s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                  {'eval_loss': inf, 'eval_wer': 0.2258313651181378, 'eval_runtime': 818.531, 'eval_samples_per_second': 9.119, 'eval_steps_per_second': 1.14, 'epoch': 18.88}
 63%|██████████████████████████████████████████▊                         | 8800/13980 [20:17:05<6:27:09,  4.48s/itSaving model checkpoint to 15_/checkpoint-8800                                                                      
Configuration saved in 15_/checkpoint-8800/config.json
Model weights saved in 15_/checkpoint-8800/pytorch_model.bin
Configuration saved in 15_/checkpoint-8800/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-8400] due to args.save_total_limit
 65%|████████████████████████████████████████████▍                       | 9141/13980 [20:52:28<6:13:12,  4.63s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
{'loss': 0.2797, 'learning_rate': 0.00010682492581602373, 'epoch': 19.74}                                          
 66%|████████████████████████████████████████████▋                       | 9200/13980 [20:58:40<7:50:07,  5.90s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                   
{'eval_loss': inf, 'eval_wer': 0.22071250886819394, 'eval_runtime': 822.5483, 'eval_samples_per_second': 9.074, 'ev 66%|████████████████████████████████████████████▋                       | 9200/13980 [21:12:23<7:50:07,  5.90s/itSaving model checkpoint to 15_/checkpoint-9200                                                                      
Configuration saved in 15_/checkpoint-9200/config.json
Model weights saved in 15_/checkpoint-9200/pytorch_model.bin
Configuration saved in 15_/checkpoint-9200/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-8800] due to args.save_total_limit
 68%|██████████████████████████████████████████████▌                     | 9569/13980 [21:50:35<5:39:29,  4.62s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
{'loss': 0.2558, 'learning_rate': 9.794510385756676e-05, 'epoch': 20.6}                                            
 69%|██████████████████████████████████████████████▋                     | 9600/13980 [21:53:58<8:50:13,  7.26s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.21949116772785646, 'eval_runtime': 826.5254, 'eval_samples_per_second': 9.031, 'eval_steps_per_second': 1.129, 'epoch': 20.6}                                                                        
 69%|██████████████████████████████████████████████▋                     | 9600/13980 [22:07:45<8:50:13,  7.26s/itSaving model checkpoint to 15_/checkpoint-9600                                                                      
Configuration saved in 15_/checkpoint-9600/config.json
Model weights saved in 15_/checkpoint-9600/pytorch_model.bin
Configuration saved in 15_/checkpoint-9600/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-9200] due to args.save_total_limit
 71%|████████████████████████████████████████████████▍                   | 9948/13980 [22:43:36<4:50:43,  4.33s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 72%|███████████████████████████████████████████████▉                   | 10000/13980 [22:49:02<7:28:43,  6.76s/it]{'loss': 0.2479, 'learning_rate': 8.906528189910978e-05, 'epoch': 21.46}
The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
 72%|███████████████████████████████████████████████▉                   | 10000/13980 [23:02:47<7:28:43,  6.76s/it]
{'eval_loss': inf, 'eval_wer': 0.22012877964670913, 'eval_runtime': 825.4563, 'eval_samples_per_second': 9.042, 'eval_steps_per_second': 1.13, 'epoch': 21.46}                                   Saving model checkpoint to 15_/checkpoint-10000                                                                     
Configuration saved in 15_/checkpoint-10000/config.json
Model weights saved in 15_/checkpoint-10000/pytorch_model.bin
Configuration saved in 15_/checkpoint-10000/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-9600] due to args.save_total_limit
 73%|█████████████████████████████████████████████████▏                 | 10258/139 73%|█████████████████████████████████████████████████▏                 | 10259/139 73%|█████████████████████████████████████████████████▏                 | 10260/139 73%|█████████████████████████████████████████████████▏                 | 10261/139 73%|█████████████████████████████████████████████████▏                 | 10262/139 73%|█████████████████████████████████████████████████▏                 | 10263/139 73%|█████████████████████████████████████████████████▏                 | 10264/139 73%|█████████████████████████████████████████████████▏                 | 10265/139 73%|█████████████████████████████████████████████████▏                 | 10266/139 73%|█████████████████████████████████████████████████▏                 | 10267/139 73%|█████████████████████████████████████████████████▏                 | 10268/139 73%|█████████████████████████████████████████████████▏                 | 10269/139 73%|█████████████████████████████████████████████████▏                 | 10270/139 73%|█████████████████████████████████████████████████▏                 | 10271/139 73%|█████████████████████████████████████████████████▏                 | 10272/139 73%|█████████████████████████████████████████████████▏                 | 10273/139 73%|█████████████████████████████████████████████████▏                 | 10274/139 73%|█████████████████████████████████████████████████▏                 | 10275/139 74%|█████████████████████████████████████████████████▏                 | 10276/13980 [23:31:33<4:50:14,  4.70s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 74%|█████████████████████████████████████████████████▎                 | 10277/139 74%|█████████████████████████████████████████████████▎                 | 10278/139 74%|█████████████████████████████████████████████████▎                 | 10279/139 74%|█████████████████████████████████████████████████▎                 | 10280/139 74%|█████████████████████████████████████████████████▎                 | 10281/139 74%|█████████████████████████████████████████████████▎                 | 10282/139 74%|█████████████████████████████████████████████████▎                 | 10283/139 74%|█████████████████████████████████████████████████▎                 | 10284/139 74%|█████████████████████████████████████████████████▎                 | 10285/139 74%|█████████████████████████████████████████████████▎                 | 10286/139 74%|█████████████████████████████████████████████████▎                 | 10287/139 74%|█████████████████████████████████████████████████▎                 | 10288/139 74%|█████████████████████████████████████████████████▎                 | 10289/139 74%|█████████████████████████████████████████████████▎                 | 10290/139 74%|█████████████████████████████████████████████████▎                 | 10291/139 74%|█████████████████████████████████████████████████▎                 | 10292/139 74%|█████████████████████████████████████████████████▎                 | 10293/139 74%|█████████████████████████████████████████████████▎                 | 10294/139 74%|█████████████████████████████████████████████████▎                 | 10295/139 74%|█████████████████████████████████████████████████▎                 | 10296/139 74%|█████████████████████████████████████████████████▎                 | 10297/139 74%|█████████████████████████████████████████████████▎                 | 10298/139 74%|█████████████████████████████████████████████████▎                 | 10299/139 74%|█████████████████████████████████████████████████▎                 | 10300/139 74%|█████████████████████████████████████████████████▎                 | 10301/139 74%|█████████████████████████████████████████████████▎                 | 10302/139 74%|█████████████████████████████████████████████████▍                 | 10303/139 74%|█████████████████████████████████████████████████▍                 | 10304/139 74%|█████████████████████████████████████████████████▍                 | 10305/139 74%|█████████████████████████████████████████████████▍                 | 10306/139 74%|█████████████████████████████████████████████████▍                 | 10307/139 74%|█████████████████████████████████████████████████▍                 | 10308/139 74%|█████████████████████████████████████████████████▍                 | 10309/139 74%|█████████████████████████████████████████████████▍                 | 10310/139 74%|█████████████████████████████████████████████████▍                 | 10311/139 74%|█████████████████████████████████████████████████▍                 | 10312/139 74%|█████████████████████████████████████████████████▍                 | 10313/139 74%|█████████████████████████████████████████████████▍                 | 10314/139 74%|█████████████████████████████████████████████████▍                 | 10315/139 74%|█████████████████████████████████████████████████▍                 | 10316/139 74%|█████████████████████████████████████████████████▍                 | 10317/139 74%|█████████████████████████████████████████████████▍                 | 10318/139 74%|█████████████████████████████████████████████████▍                 | 10319/139 74%|█████████████████████████████████████████████████▍                 | 10320/139 74%|█████████████████████████████████████████████████▍                 | 10321/139 74%|█████████████████████████████████████████████████▍                 | 10322/139 74%|█████████████████████████████████████████████████▍                 | 10323/139 74%|█████████████████████████████████████████████████▍                 | 10324/139 74%|█████████████████████████████████████████████████▍                 | 10325/139 74%|█████████████████████████████████████████████████▍                 | 10326/139 74%|█████████████████████████████████████████████████▍                 | 10327/139 74%|█████████████████████████████████████████████████▍                 | 10328/139 74%|█████████████████████████████████████████████████▌                 | 10329/139 74%|█████████████████████████████████████████████████▌                 | 10330/139 74%|█████████████████████████████████████████████████▌                 | 10331/139 74%|█████████████████████████████████████████████████▌                 | 10332/139 74%|█████████████████████████████████████████████████▌                 | 10333/139 74%|█████████████████████████████████████████████████▌                 | 10334/139 74%|█████████████████████████████████████████████████▌                 | 10335/139 74%|█████████████████████████████████████████████████▌                 | 10336/139 74%|█████████████████████████████████████████████████▌                 | 10337/139 74%|█████████████████████████████████████████████████▌                 | 10338/139 74%|█████████████████████████████████████████████████▌                 | 10339/139 74%|█████████████████████████████████████████████████▌                 | 10340/139 74%|█████████████████████████████████████████████████▌                 | 10341/139 74%|█████████████████████████████████████████████████▌                 | 10342/139 74%|█████████████████████████████████████████████████▌                 | 10343/139 74%|█████████████████████████████████████████████████▌                 | 10344/139 74%|█████████████████████████████████████████████████▌                 | 10345/139 74%|█████████████████████████████████████████████████▌                 | 10346/139 74%|█████████████████████████████████████████████████▌                 | 10347/139 74%|█████████████████████████████████████████████████▌                 | 10348/139 74%|█████████████████████████████████████████████████▌                 | 10349/139 74%|█████████████████████████████████████████████████▌                 | 10350/139 74%|█████████████████████████████████████████████████▌                 | 10351/139 74%|█████████████████████████████████████████████████▌                 | 10352/139 74%|█████████████████████████████████████████████████▌                 | 10353/139 74%|█████████████████████████████████████████████████▌                 | 10354/139 74%|█████████████████████████████████████████████████▋                 | 10355/139 74%|█████████████████████████████████████████████████▋                 | 10356/139 74%|█████████████████████████████████████████████████▋                 | 10357/139 74%|█████████████████████████████████████████████████▋                 | 10358/139 74%|█████████████████████████████████████████████████▋                 | 10359/139 74%|█████████████████████████████████████████████████▋                 | 10360/139 74%|█████████████████████████████████████████████████▋                 | 10361/139 74%|█████████████████████████████████████████████████▋                 | 10362/139 74%|█████████████████████████████████████████████████▋                 | 10363/139 74%|█████████████████████████████████████████████████▋                 | 10364/139 74%|█████████████████████████████████████████████████▋                 | 10365/139 74%|█████████████████████████████████████████████████▋                 | 10366/139 74%|█████████████████████████████████████████████████▋                 | 10367/139 74%|█████████████████████████████████████████████████▋                 | 10368/139 74%|█████████████████████████████████████████████████▋                 | 10369/139 74%|█████████████████████████████████████████████████▋                 | 10370/139 74%|█████████████████████████████████████████████████▋                 | 10371/139 74%|█████████████████████████████████████████████████▋                 | 10372/139 74%|█████████████████████████████████████████████████▋                 | 10373/139 74%|█████████████████████████████████████████████████▋                 | 10374/139 74%|█████████████████████████████████████████████████▋                 | 10375/139 74%|█████████████████████████████████████████████████▋                 | 10376/139 74%|█████████████████████████████████████████████████▋                 | 10377/139 74%|█████████████████████████████████████████████████▋                 | 10378/139 74%|█████████████████████████████████████████████████▋                 | 10379/139 74%|█████████████████████████████████████████████████▋                 | 10380/139 74%|█████████████████████████████████████████████████▊                 | 10381/139 74%|█████████████████████████████████████████████████▊                 | 10382/139 74%|█████████████████████████████████████████████████▊                 | 10383/139 74%|█████████████████████████████████████████████████▊                 | 10384/139 74%|█████████████████████████████████████████████████▊                 | 10385/139 74%|█████████████████████████████████████████████████▊                 | 10386/139 74%|█████████████████████████████████████████████████▊                 | 10387/139 74%|█████████████████████████████████████████████████▊                 | 10388/139 74%|█████████████████████████████████████████████████▊                 | 10389/139 74%|█████████████████████████████████████████████████▊                 | 10390/139 74%|█████████████████████████████████████████████████▊                 | 10391/139 74%|█████████████████████████████████████████████████▊                 | 10392/139 74%|█████████████████████████████████████████████████▊                 | 10393/139 74%|█████████████████████████████████████████████████▊                 | 10394/139 74%|█████████████████████████████████████████████████▊                 | 10395/139 74%|█████████████████████████████████████████████████▊                 | 10396/139 74%|█████████████████████████████████████████████████▊                 | 10397/139 74%|█████████████████████████████████████████████████▊                 | 10398/139 74%|█████████████████████████████████████████████████▊                 | 10399/139 74%|█████████████████████████████████████████████████▊                 | 10400/139                                                                                   {'loss': 0.2393, 'learning_rate': 8.018545994065282e-05, 'epoch': 22.32}
 74%|█████████████████████████████████████████████████▊                 | 10400/13980 [23:44:31<5:10:06,  5.20s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                   █████████████████| 933/933 [13:34<00:00,  1.41it/s]
{'eval_loss': inf, 'eval_wer': 0.21910500839672034, 'eval_runtime': 823.5975, 'eval 74%|█████████████████████████████████████████████████▊                 | 10400/13980 [23:58:14<5:10:06,  5.20s/it]                                                  Saving model checkpoint to 15_/checkpoint-10400                                     
Configuration saved in 15_/checkpoint-10400/config.json
Model weights saved in 15_/checkpoint-10400/pytorch_model.bin
Configuration saved in 15_/checkpoint-10400/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-10000] due to args.save_total_limit
 74%|███████████████████████████████████████████████▌                | 10401/13980  74%|███████████████████████████████████████████████▌                | 10402/13980  74%|███████████████████████████████████████████████▌                | 10403/13980  74%|█████████████████████████████████████████████████                 | 10404/1398 74%|█████████████████████████████████████████████████                 | 10405/1398 74%|█████████████████████████████████████████████████▏                | 10406/1398 74%|█████████████████████████████████████████████████▏                | 10407/1398 74%|█████████████████████████████████████████████████▏                | 10408/1398 74%|█████████████████████████████████████████████████▏                | 10409/1398 74%|█████████████████████████████████████████████████▏                | 10410/1398 74%|█████████████████████████████████████████████████▏                | 10411/1398 74%|█████████████████████████████████████████████████▏                | 10412/1398 74%|█████████████████████████████████████████████████▉                 | 10413/139 74%|█████████████████████████████████████████████████▉                 | 10414/139 74%|█████████████████████████████████████████████████▉                 | 10415/139 75%|█████████████████████████████████████████████████▉                 | 10416/139 75%|█████████████████████████████████████████████████▉                 | 10417/139 75%|█████████████████████████████████████████████████▉                 | 10418/139 75%|█████████████████████████████████████████████████▉                 | 10419/139 75%|█████████████████████████████████████████████████▉                 | 10420/139 75%|█████████████████████████████████████████████████▉                 | 10421/139 75%|█████████████████████████████████████████████████▉                 | 10422/139 75%|█████████████████████████████████████████████████▉                 | 10423/139 75%|█████████████████████████████████████████████████▉                 | 10424/139 75%|█████████████████████████████████████████████████▉                 | 10425/139 75%|█████████████████████████████████████████████████▉                 | 10426/139 75%|█████████████████████████████████████████████████▉                 | 10427/139 75%|█████████████████████████████████████████████████▉                 | 10428/139 75%|█████████████████████████████████████████████████▉                 | 10429/139 75%|█████████████████████████████████████████████████▉                 | 10430/139 75%|█████████████████████████████████████████████████▉                 | 10431/139 75%|█████████████████████████████████████████████████▉                 | 10432/139 75%|██████████████████████████████████████████████████                 | 10433/139 75%|██████████████████████████████████████████████████                 | 10434/139 75%|██████████████████████████████████████████████████                 | 10435/139 75%|██████████████████████████████████████████████████                 | 10436/139 75%|██████████████████████████████████████████████████                 | 10437/139 75%|██████████████████████████████████████████████████                 | 10438/139 75%|██████████████████████████████████████████████████                 | 10439/139 75%|██████████████████████████████████████████████████                 | 10440/139 75%|██████████████████████████████████████████████████                 | 10441/139 75%|██████████████████████████████████████████████████                 | 10442/139 75%|██████████████████████████████████████████████████                 | 10443/139 75%|██████████████████████████████████████████████████                 | 10444/139 75%|██████████████████████████████████████████████████                 | 10445/139 75%|██████████████████████████████████████████████████                 | 10446/139 75%|██████████████████████████████████████████████████                 | 10447/139 75%|██████████████████████████████████████████████████                 | 10448/139 75%|██████████████████████████████████████████████████                 | 10449/139 75%|██████████████████████████████████████████████████                 | 10450/139 75%|██████████████████████████████████████████████████                 | 10451/139 75%|██████████████████████████████████████████████████                 | 10452/139 75%|██████████████████████████████████████████████████                 | 10453/139 75%|██████████████████████████████████████████████████                 | 10454/139 75%|██████████████████████████████████████████████████                 | 10455/139 75%|██████████████████████████████████████████████████                 | 10456/139 75%|██████████████████████████████████████████████████                 | 10457/139 75%|██████████████████████████████████████████████████                 | 10458/139 75%|██████████████████████████████████████████████████▏                | 10459/139 75%|██████████████████████████████████████████████████▏                | 10460/139 75%|██████████████████████████████████████████████████▏                | 10461/139 75%|██████████████████████████████████████████████████▏                | 10462/139 75%|██████████████████████████████████████████████████▏                | 10463/139 75%|██████████████████████████████████████████████████▏                | 10464/139 75%|██████████████████████████████████████████████████▏                | 10465/139 75%|██████████████████████████████████████████████████▏                | 10466/139 75%|██████████████████████████████████████████████████▏                | 10467/139 75%|██████████████████████████████████████████████████▏                | 10468/139 75%|██████████████████████████████████████████████████▏                | 10469/139 75%|██████████████████████████████████████████████████▏                | 10470/139 75%|██████████████████████████████████████████████████▏                | 10471/139 75%|██████████████████████████████████████████████████▏                | 10472/139 75%|██████████████████████████████████████████████████▏                | 10473/139 75%|██████████████████████████████████████████████████▏                | 10474/139 75%|██████████████████████████████████████████████████▏                | 10475/139 75%|██████████████████████████████████████████████████▏                | 10476/139 75%|██████████████████████████████████████████████████▏                | 10477/139 75%|██████████████████████████████████████████████████▏                | 10478/139 75%|██████████████████████████████████████████████████▏                | 10479/139 75%|██████████████████████████████████████████████████▏                | 10480/139 75%|██████████████████████████████████████████████████▏                | 10481/139 75%|██████████████████████████████████████████████████▏                | 10482/139 75%|██████████████████████████████████████████████████▏                | 10483/139 75%|██████████████████████████████████████████████████▏                | 10484/139 75%|██████████████████████████████████████████████████▎                | 10485/139 75%|██████████████████████████████████████████████████▎                | 10486/139 75%|██████████████████████████████████████████████████▎                | 10487/139 75%|██████████████████████████████████████████████████▎                | 10488/139 75%|██████████████████████████████████████████████████▎                | 10489/139 75%|██████████████████████████████████████████████████▎                | 10490/139 75%|██████████████████████████████████████████████████▎                | 10491/139 75%|██████████████████████████████████████████████████▎                | 10492/139 75%|██████████████████████████████████████████████████▎                | 10493/139 75%|██████████████████████████████████████████████████▎                | 10494/139 75%|██████████████████████████████████████████████████▎                | 10495/139 75%|██████████████████████████████████████████████████▎                | 10496/139 75%|██████████████████████████████████████████████████▎                | 10497/139 75%|██████████████████████████████████████████████████▎                | 10498/139 75%|██████████████████████████████████████████████████▎                | 10499/139 75%|██████████████████████████████████████████████████▎                | 10500/139 75%|██████████████████████████████████████████████████▎                | 10501/139 75%|██████████████████████████████████████████████████▎                | 10502/139 75%|██████████████████████████████████████████████████▎                | 10503/139 75%|██████████████████████████████████████████████████▎                | 10504/139 75%|██████████████████████████████████████████████████▎                | 10505/139 75%|██████████████████████████████████████████████████▎                | 10506/139 75%|██████████████████████████████████████████████████▎                | 10507/139 75%|██████████████████████████████████████████████████▎                | 10508/139 75%|██████████████████████████████████████████████████▎                | 10509/139 75%|██████████████████████████████████████████████████▎                | 10510/139 75%|██████████████████████████████████████████████████▎                | 10511/139 75%|██████████████████████████████████████████████████▍                | 10512/139 75%|██████████████████████████████████████████████████▍                | 10513/139 75%|██████████████████████████████████████████████████▍                | 10514/139 75%|██████████████████████████████████████████████████▍                | 10515/139 75%|██████████████████████████████████████████████████▍                | 10516/139 75%|██████████████████████████████████████████████████▍                | 10517/139 75%|██████████████████████████████████████████████████▍                | 10518/139 75%|██████████████████████████████████████████████████▍                | 10519/139 75%|██████████████████████████████████████████████████▍                | 10520/139 75%|██████████████████████████████████████████████████▍                | 10521/139 75%|██████████████████████████████████████████████████▍                | 10522/139 75%|██████████████████████████████████████████████████▍                | 10523/139 75%|██████████████████████████████████████████████████▍                | 10524/139 75%|██████████████████████████████████████████████████▍                | 10525/139 75%|██████████████████████████████████████████████████▍                | 10526/139 75%|██████████████████████████████████████████████████▍                | 10527/139 75%|██████████████████████████████████████████████████▍                | 10528/139 75%|██████████████████████████████████████████████████▍                | 10529/139 75%|██████████████████████████████████████████████████▍                | 10530/139 75%|██████████████████████████████████████████████████▍                | 10531/139 75%|██████████████████████████████████████████████████▍                | 10532/139 75%|██████████████████████████████████████████████████▍                | 10533/139 75%|██████████████████████████████████████████████████▍                | 10534/139 75%|██████████████████████████████████████████████████▍                | 10535/139 75%|██████████████████████████████████████████████████▍                | 10536/139 75%|██████████████████████████████████████████████████▍                | 10537/139 75%|██████████████████████████████████████████████████▌                | 10538/139 75%|██████████████████████████████████████████████████▌                | 10539/139 75%|██████████████████████████████████████████████████▌                | 10540/139 75%|██████████████████████████████████████████████████▌                | 10541/139 75%|██████████████████████████████████████████████████▌                | 10542/139 75%|██████████████████████████████████████████████████▌                | 10543/139 75%|██████████████████████████████████████████████████▌                | 10544/139 75%|██████████████████████████████████████████████████▌                | 10545/139 75%|██████████████████████████████████████████████████▌                | 10546/139 75%|██████████████████████████████████████████████████▌                | 10547/139 75%|██████████████████████████████████████████████████▌                | 10548/139 75%|██████████████████████████████████████████████████▌                | 10549/139 75%|██████████████████████████████████████████████████▌                | 10550/139 75%|██████████████████████████████████████████████████▌                | 10551/139 75%|██████████████████████████████████████████████████▌                | 10552/139 75%|██████████████████████████████████████████████████▌                | 10553/139 75%|██████████████████████████████████████████████████▌                | 10554/139 76%|██████████████████████████████████████████████████▌                | 10555/139 76%|██████████████████████████████████████████████████▌                | 10556/139 76%|██████████████████████████████████████████████████▌                | 10557/139 76%|██████████████████████████████████████████████████▌                | 10558/139 76%|██████████████████████████████████████████████████▌                | 10559/139 76%|██████████████████████████████████████████████████▌                | 10560/139 76%|██████████████████████████████████████████████████▌                | 10561/139 76%|██████████████████████████████████████████████████▌                | 10562/139 76%|██████████████████████████████████████████████████▌                | 10563/139 76%|██████████████████████████████████████████████████▋                | 10564/139 76%|██████████████████████████████████████████████████▋                | 10565/139 76%|██████████████████████████████████████████████████▋                | 10566/139 76%|██████████████████████████████████████████████████▋                | 10567/139 76%|██████████████████████████████████████████████████▋                | 10568/139 76%|██████████████████████████████████████████████████▋                | 10569/139 76%|██████████████████████████████████████████████████▋                | 10570/139 76%|██████████████████████████████████████████████████▋                | 10571/139 76%|██████████████████████████████████████████████████▋                | 10572/139 76%|██████████████████████████████████████████████████▋                | 10573/139 76%|██████████████████████████████████████████████████▋                | 10574/139 76%|██████████████████████████████████████████████████▋                | 10575/139 76%|██████████████████████████████████████████████████▋                | 10576/139 76%|██████████████████████████████████████████████████▋                | 10577/139 76%|██████████████████████████████████████████████████▋                | 10578/139 76%|██████████████████████████████████████████████████▋                | 10579/139 76%|██████████████████████████████████████████████████▋                | 10580/139 76%|██████████████████████████████████████████████████▋                | 10581/139 76%|██████████████████████████████████████████████████▋                | 10582/139 76%|██████████████████████████████████████████████████▋                | 10583/139 76%|██████████████████████████████████████████████████▋                | 10584/139 76%|██████████████████████████████████████████████████▋                | 10585/139 76%|██████████████████████████████████████████████████▋                | 10586/139 76%|██████████████████████████████████████████████████▋                | 10587/139 76%|██████████████████████████████████████████████████▋                | 10588/139 76%|██████████████████████████████████████████████████▋                | 10589/139 76%|██████████████████████████████████████████████████▊                | 10590/139 76%|██████████████████████████████████████████████████▊                | 10591/139 76%|██████████████████████████████████████████████████▊                | 10592/139 76%|██████████████████████████████████████████████████▊                | 10593/139 76%|██████████████████████████████████████████████████▊                | 10594/139 76%|██████████████████████████████████████████████████▊                | 10595/139 76%|██████████████████████████████████████████████████▊                | 10596/139 76%|██████████████████████████████████████████████████▊                | 10597/13980  76%|██████████████████████████████████████████████████▊                | 10598/13980 [24:18:46<5:31 76%|██████████████████████████████████████████████████▊                | 10599/13980 [24:18:51<5:14 76%|██████████████████████████████████████████████████▊                | 10600/13980 [24:18:55<4:55:51,  5.25s/ 77%|███████████████████████████████████████████████████▌               | 10767/13980 [24:36:12<4:16:31,  4.79s/it]/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 77%|███████████████████████████████████████████████████▊               | 10800/13980 [24:39:43<5:48:37,  6.58s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
{'loss': 0.234, 'learning_rate': 7.130563798219584e-05, 'epoch': 23.18}
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                   
                                                                                                                                                                                                   77%|███████████████████████████████████████████████████▊               | 10800/13980 [24:53:23<5:48:37,  6.58s/it]al_steps_per_second': 1.137, 'epoch': 23.18}                                  Saving model checkpoint to 15_/checkpoint-10800                                                                                                                                                    
Configuration saved in 15_/checkpoint-10800/config.json
Model weights saved in 15_/checkpoint-10800/pytorch_model.bin
Configuration saved in 15_/checkpoint-10800/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-10400] due to args.save_total_limit
 80%|█████████████████████████████████████████████████████▋             | 11200/13980 [25:34:39<5:34:06,  7.21s/it]{'loss': 0.2367, 'learning_rate': 6.240356083086053e-05, 'epoch': 24.03}
 80%|█████████████████████████████████████████████████████▋             | 11200/13980 [25:34:39<5:34:06,  7.21s/it]The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.
***** Running Evaluation *****
  Num examples = 7464
  Batch size = 8
                                                                                                                                                                                                 {'eval_loss': inf, 'eval_wer': 0.2183147288353255, 'eval_runtime': 813.1443, 'eval_samples_per_second': 9.179, 'eval_steps_per_second': 1.147, 'epoch': 24.03}███| 933/933 [13:23<00:00,  1.43it/s]
 80%|█████████████████████████████████████████████████████▋             | 11200/13980 [25:48:12<5:34:06,  7.21s/it]                                                                              Saving model checkpoint to 15_/checkpoint-11200                                                                                                                                                    
Configuration saved in 15_/checkpoint-11200/config.json
Model weights saved in 15_/checkpoint-11200/pytorch_model.bin
Configuration saved in 15_/checkpoint-11200/preprocessor_config.json
Deleting older checkpoint [15_/checkpoint-10800] due to args.save_total_limit
 80%|████████████████████████████████████████████████████▉             | 11209/13980 [25:49:01<13:48:31, 17.94s/it]